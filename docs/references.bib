@inproceedings{A.O.T2022,
  title = {Slicing {{Aided Hyper Inference}} and {{Fine-Tuning}} for {{Small Object Detection}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Akyon, Fatih Cagatay and Onur Altinuc, Sinan and Temizel, Alptekin},
  year = {2022},
  month = oct,
  pages = {966--970},
  issn = {2381-8549},
  doi = {10.1109/ICIP46576.2022.9897990},
  urldate = {2023-10-16},
  abstract = {Detection of small objects and objects far away in the scene is a major challenge in surveillance applications. Such objects are represented by small number of pixels in the image and lack sufficient details, making them difficult to detect using conventional detectors. In this work, an open-source framework called Slicing Aided Hyper Inference (SAHI) is proposed that provides a generic slicing aided inference and fine-tuning pipeline for small object detection. The proposed technique is generic in the sense that it can be applied on top of any available object detector without any fine-tuning. Experimental evaluations, using object detection baselines on the Visdrone and xView aerial object detection datasets show that the proposed inference method can increase object detection AP by 6.8\%, 5.1\% and 5.3\% for FCOS, VFNet and TOOD detectors, respectively. Moreover, the detection accuracy can be further increased with a slicing aided fine-tuning, resulting in a cumulative increase of 12.7\%, 13.4\% and 14.5\% AP in the same order. Proposed technique has been integrated with Detectron2, MMDetection and YOLOv5 models and it is publicly available at https://github.com/obss/sahi.git},
  file = {/home/camille/Zotero/storage/SBS3E5E5/Akyon et al. - 2022 - Slicing Aided Hyper Inference and Fine-Tuning for .html}
}

@inproceedings{A.O.T2022a,
  title = {Slicing {{Aided Hyper Inference}} and {{Fine-Tuning}} for {{Small Object Detection}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Akyon, Fatih Cagatay and Onur Altinuc, Sinan and Temizel, Alptekin},
  year = {2022},
  month = oct,
  pages = {966--970},
  issn = {2381-8549},
  doi = {10.1109/ICIP46576.2022.9897990},
  urldate = {2023-12-04},
  abstract = {Detection of small objects and objects far away in the scene is a major challenge in surveillance applications. Such objects are represented by small number of pixels in the image and lack sufficient details, making them difficult to detect using conventional detectors. In this work, an open-source framework called Slicing Aided Hyper Inference (SAHI) is proposed that provides a generic slicing aided inference and fine-tuning pipeline for small object detection. The proposed technique is generic in the sense that it can be applied on top of any available object detector without any fine-tuning. Experimental evaluations, using object detection baselines on the Visdrone and xView aerial object detection datasets show that the proposed inference method can increase object detection AP by 6.8\%, 5.1\% and 5.3\% for FCOS, VFNet and TOOD detectors, respectively. Moreover, the detection accuracy can be further increased with a slicing aided fine-tuning, resulting in a cumulative increase of 12.7\%, 13.4\% and 14.5\% AP in the same order. Proposed technique has been integrated with Detectron2, MMDetection and YOLOv5 models and it is publicly available at https://github.com/obss/sahi.git},
  file = {/home/camille/Zotero/storage/8I2CSANK/Akyon et al. - 2022 - Slicing Aided Hyper Inference and Fine-Tuning for .html}
}

@article{Arnett2020,
  title = {Race, {{Surveillance}}, {{Resistance}}},
  author = {Arnett, Chaz},
  year = {2020},
  journal = {Ohio State Law Journal},
  volume = {81},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/camille/Zotero/storage/D5LHMU53/Arnett - Race, Surveillance, Resistance.pdf}
}

@article{Boeing2017,
  title = {{{OSMnx}}: {{New}} Methods for Acquiring, Constructing, Analyzing, and Visualizing Complex Street Networks},
  shorttitle = {{{OSMnx}}},
  author = {Boeing, Geoff},
  year = {2017},
  month = sep,
  journal = {Computers, Environment and Urban Systems},
  volume = {65},
  pages = {126--139},
  issn = {0198-9715},
  doi = {10.1016/j.compenvurbsys.2017.05.004},
  urldate = {2023-09-25},
  abstract = {Urban scholars have studied street networks in various ways, but there are data availability and consistency limitations to the current urban planning/street network analysis literature. To address these challenges, this article presents OSMnx, a new tool to make the collection of data and creation and analysis of street networks simple, consistent, automatable and sound from the perspectives of graph theory, transportation, and urban design. OSMnx contributes five significant capabilities for researchers and practitioners: first, the automated downloading of political boundaries and building footprints; second, the tailored and automated downloading and constructing of street network data from OpenStreetMap; third, the algorithmic correction of network topology; fourth, the ability to save street networks to disk as shapefiles, GraphML, or SVG files; and fifth, the ability to analyze street networks, including calculating routes, projecting and visualizing networks, and calculating metric and topological measures. These measures include those common in urban design and transportation studies, as well as advanced measures of the structure and topology of the network. Finally, this article presents a simple case study using OSMnx to construct and analyze street networks in Portland, Oregon.},
  keywords = {Complex networks,GIS,OpenStreetMap,Python,Resilience,Street network,Transportation,Urban design,Urban form,Visualization},
  file = {/home/camille/Zotero/storage/X78V34YP/Boeing_2017_OSMnx.pdf;/home/camille/Zotero/storage/I5PZIJIB/S0198971516303970.html}
}

@book{Browne2015,
  title = {Dark {{Matters}}: {{On}} the {{Surveillance}} of {{Blackness}}},
  shorttitle = {Dark {{Matters}}},
  author = {Browne, Simone},
  year = {2015},
  publisher = {{Duke University Press}},
  address = {{Durham, NC}},
  abstract = {In Dark Matters Simone Browne locates the conditions of blackness as a key site through which surveillance is practiced, narrated, and resisted. She shows how contemporary surveillance technologies and practices are informed by the long history of racial formation and by the methods of policing black life under slavery, such as branding, runaway slave notices, and lantern laws. Placing surveillance studies into conversation with the archive of transatlantic slavery and its afterlife, Browne draws from black feminist theory, sociology, and cultural studies to analyze texts as diverse as the methods of surveilling blackness she discusses: from the design of the eighteenth-century slave ship Brooks, Jeremy Bentham's Panopticon, and The Book of Negroes, to contemporary art, literature, biometrics, and post-9/11 airport security practices. Surveillance, Browne asserts, is both a discursive and material practice that reifies boundaries, borders, and bodies around racial lines, so much so that the surveillance of blackness has long been, and continues to be, a social and political norm.},
  isbn = {978-0-8223-5938-8},
  file = {/home/camille/Zotero/storage/KKPAL283/dark-matters.html}
}

@article{Chiania,
  title = {Leaders of a {{Beautiful Struggle}} v. {{Baltimore Police Department}}:{{Balancing}} the {{Advances}} of {{Police Tracking Technology}} with the {{Constitutional Rights Afforded}} to the {{Public Citizenry}}},
  author = {Chiani, Cameron},
  journal = {Journal of Business},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/camille/Zotero/storage/I9TPFXCL/Chiani - Leaders of a Beautiful Struggle v. Baltimore Polic.pdf}
}

@misc{detectron,
  title = {Detectron2},
  author = {Wu, Yuxin and Kirillov, Alexander and Massa, Francisco and Lo, Wan-Yen and Girshick, Ross},
  year = {2019},
  url = {https://github.com/facebookresearch/detectron2}
}

@book{Gilmore2023,
  title = {Abolition {{Geography}}},
  author = {Gilmore, Ruth Wilson},
  year = {2023},
  urldate = {2023-09-16},
  abstract = {The first collection of writings from one of the foremost contemporary critical thinkers on racism, geography and incarceration  Gathering together Ruth Wilson Gilmore's work from over three decades,...},
  langid = {american},
  file = {/home/camille/Zotero/storage/XXGDHXYR/abolition-geography-by-ruth-wilson-gilmore.html}
}

@misc{J.C.Q2023,
  title = {{{YOLO}} by {{Ultralytics}}},
  author = {Jocher, Glenn and Chaurasia, Ayush and Qiu, Jing},
  year = {2023},
  month = jan,
  url = {https://github.com/ultralytics/ultralytics},
  copyright = {AGPL-3.0}
}

@article{Joh2017,
  title = {The {{Undue Influence}} of {{Surveillance Technology Companies}} on {{Policing}}},
  author = {Joh, Elizabeth E.},
  year = {2017},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.2924620},
  urldate = {2023-09-11},
  langid = {english},
  file = {/home/camille/Zotero/storage/X7JB449Q/Joh - 2017 - The Undue Influence of Surveillance Technology Com.pdf}
}

@misc{L.Z.X+2023,
  title = {{{DETRs Beat YOLOs}} on {{Real-time Object Detection}}},
  author = {Lv, Wenyu and Zhao, Yian and Xu, Shangliang and Wei, Jinman and Wang, Guanzhong and Cui, Cheng and Du, Yuning and Dang, Qingqing and Liu, Yi},
  year = {2023},
  month = jul,
  number = {arXiv:2304.08069},
  eprint = {2304.08069},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.08069},
  urldate = {2023-12-03},
  abstract = {Recently, end-to-end transformer-based detectors{\textasciitilde}(DETRs) have achieved remarkable performance. However, the issue of the high computational cost of DETRs has not been effectively addressed, limiting their practical application and preventing them from fully exploiting the benefits of no post-processing, such as non-maximum suppression (NMS). In this paper, we first analyze the influence of NMS in modern real-time object detectors on inference speed, and establish an end-to-end speed benchmark. To avoid the inference delay caused by NMS, we propose a Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge. Specifically, we design an efficient hybrid encoder to efficiently process multi-scale features by decoupling the intra-scale interaction and cross-scale fusion, and propose IoU-aware query selection to improve the initialization of object queries. In addition, our proposed detector supports flexibly adjustment of the inference speed by using different decoder layers without the need for retraining, which facilitates the practical application of real-time object detectors. Our RT-DETR-L achieves 53.0\% AP on COCO val2017 and 114 FPS on T4 GPU, while RT-DETR-X achieves 54.8\% AP and 74 FPS, outperforming all YOLO detectors of the same scale in both speed and accuracy. Furthermore, our RT-DETR-R50 achieves 53.1\% AP and 108 FPS, outperforming DINO-Deformable-DETR-R50 by 2.2\% AP in accuracy and by about 21 times in FPS. ource code and pre-trained models are available at https://github.com/lyuwenyu/RT-DETR.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/camille/Zotero/storage/MG75NPN9/Lv et al. - 2023 - DETRs Beat YOLOs on Real-time Object Detection.html}
}

@inproceedings{N.O.R+2017,
  title = {The {{Mapillary Vistas Dataset}} for {{Semantic Understanding}} of {{Street Scenes}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Neuhold, Gerhard and Ollmann, Tobias and Rota Bulo, Samuel and Kontschieder, Peter},
  year = {2017},
  pages = {4990--4999},
  url = {https://openaccess.thecvf.com/content_iccv_2017/html/Neuhold_The_Mapillary_Vistas_ICCV_2017_paper.html},
  urldate = {2023-12-03},
  file = {/home/camille/Zotero/storage/8GTNX5KC/Neuhold et al_2017_The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes.pdf}
}

@misc{R.D.G+2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = may,
  number = {arXiv:1506.02640},
  eprint = {1506.02640},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1506.02640},
  urldate = {2023-12-03},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/camille/Zotero/storage/3F2BE465/Redmon et al_2016_You Only Look Once.pdf;/home/camille/Zotero/storage/ATK4SGNJ/1506.html}
}

@incollection{Robbins2022,
  title = {Machine {{Learning}}, {{Mass Surveillance}}, and {{National Security}}: {{Data}}, {{Efficacy}}, and {{Meaningful Human Control}}},
  shorttitle = {Machine {{Learning}}, {{Mass Surveillance}}, and {{National Security}}},
  booktitle = {The {{Palgrave Handbook}} of {{National Security}}},
  author = {Robbins, Scott},
  editor = {Clarke, Michael and Henschke, Adam and Sussex, Matthew and Legrand, Tim},
  year = {2022},
  pages = {371--388},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-53494-3_16},
  urldate = {2023-09-11},
  abstract = {This chapter highlights the many issues confronting the use of machine learning (ML) in the context of national security and mass surveillance, with a focus on the ethical challenges posed by ML. This attention to ethics serves two purposes. First, given this fast-moving field, an ethical focus allows for broader recognition of the issues arising from ML{\textemdash}even though particular aspects of the technology will continue to change, many of these ethical issues will remain constant. Second, these issues are chosen because they are important{\textemdash}not only are these likely to be ongoing issues for national security, they warrant concern and attention. The first set of issues stems from the data used to train ML algorithms and the second set of issues concerns the efficacy of ML, without which it would be unethical to use. Finally, due to the opacity of the features causing a particular ML decision, there are social issues surrounding human accountability and responsibility when something goes wrong. That is, how do we ensure that humans are in meaningful control over ML algorithms that could result in ethically salient consequences? The chapter concludes that while navigating all of these issues will be difficult, knowing that they exist is the first step toward ensuring that ML is implemented for mass surveillance in a way consistent with liberal democratic values.},
  isbn = {978-3-030-53494-3},
  langid = {english}
}

@inproceedings{S.L.Z+2019,
  title = {Objects365: {{A Large-Scale}}, {{High-Quality Dataset}} for {{Object Detection}}},
  shorttitle = {Objects365},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Shao, Shuai and Li, Zeming and Zhang, Tianyuan and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Li, Jing and Sun, Jian},
  year = {2019},
  month = oct,
  pages = {8429--8438},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2019.00852},
  abstract = {In this paper, we introduce a new large-scale object detection dataset, Objects365, which has 365 object categories over 600K training images. More than 10 million, high-quality bounding boxes are manually labeled through a three-step, carefully designed annotation pipeline. It is the largest object detection dataset (with full annotation) so far and establishes a more challenging benchmark for the community. Objects365 can serve as a better feature learning dataset for localization-sensitive tasks like object detection and semantic segmentation. The Objects365 pre-trained models significantly outperform ImageNet pre-trained models with 5.6 points gain (42 vs 36.4) based on the standard setting of 90K iterations on COCO benchmark. Even compared with much long training time like 540K iterations, our Objects365 pretrained model with 90K iterations still have 2.7 points gain (42 vs 39.3). Meanwhile, the finetuning time can be greatly reduced (up to 10 times) when reaching the same accuracy. Better generalization ability of Object365 has also been verified on CityPersons, VOC segmentation, and ADE tasks. The dataset as well as the pretrained-models have been released at www.objects365.org.},
  keywords = {Benchmark testing,Clocks,Detectors,Object detection,Pipelines,Task analysis,Training},
  file = {/home/camille/Zotero/storage/MG3T3UH9/9009553.html}
}

@inproceedings{S.Y.G2021,
  title = {Surveilling {{Surveillance}}: {{Estimating}} the {{Prevalence}} of {{Surveillance Cameras}} with {{Street View Data}}},
  shorttitle = {Surveilling {{Surveillance}}},
  booktitle = {Proceedings of the 2021 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Sheng, Hao and Yao, Keniel and Goel, Sharad},
  year = {2021},
  month = jul,
  series = {{{AIES}} '21},
  pages = {221--230},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3461702.3462525},
  urldate = {2023-03-28},
  abstract = {The use of video surveillance in public spaces--both by government agencies and by private citizens--has attracted considerable attention in recent years, particularly in light of rapid advances in face-recognition technology. But it has been difficult to systematically measure the prevalence and placement of cameras, hampering efforts to assess the implications of surveillance on privacy and public safety. Here we present a novel approach for estimating the spatial distribution of surveillance cameras: applying computer vision algorithms to large-scale street view image data. Specifically, we build a camera detection model and apply it to 1.6 million street view images sampled from 10 large U.S. cities and 6 other major cities around the world, with positive model detections verified by human experts. After adjusting for the estimated recall of our model, and accounting for the spatial coverage of our sampled images, we are able to estimate the density of surveillance cameras visible from the road. Across the 16 cities we consider, the estimated number of surveillance cameras per linear kilometer ranges from 0.1 (in Seattle) to 0.9 (in Seoul). In a detailed analysis of the 10 U.S. cities, we find that cameras are concentrated in commercial, industrial, and mixed zones, and in neighborhoods with higher shares of non-white residents---a pattern that persists even after adjusting for land use. These results help inform ongoing discussions on the use of surveillance technology, including its potential disparate impacts on communities of color.},
  isbn = {978-1-4503-8473-5},
  keywords = {computer vision,privacy,surveillance,urban computing},
  file = {/home/camille/Zotero/storage/EZL59AES/Sheng et al. - 2021 - Surveilling Surveillance Estimating the Prevalenc.pdf}
}

@article{Snyder2020a,
  title = {``{{Big Brother}}'s {{Bigger Brother}}'': {{The Visual Politics}} of ({{Counter}}) {{Surveillance}} in {{Baltimore}}},
  shorttitle = {``{{Big Brother}}'s {{Bigger Brother}}''},
  author = {Snyder, Benjamin H.},
  year = {2020},
  month = dec,
  journal = {Sociological Forum},
  volume = {35},
  number = {4},
  pages = {1315--1336},
  issn = {0884-8971, 1573-7861},
  doi = {10.1111/socf.12649},
  urldate = {2023-09-16},
  abstract = {In 2016, without the knowledge of its citizens, Baltimore City Police deployed a military aerial surveillance technology called Wide Area Motion Imagery (WAMI), which can track the movements of every person in public view over the entire city. Though the trial of the ``spy plane,'' as the program was dubbed, quickly ended in scandal, organizers from Baltimore's low-income minority neighborhoods successfully rebooted the program in 2020, this time framing WAMI partly as a tool of ``sousveillance'' (watching ``from below'') that can track the movements of police officers. The paper shows how organizers ``rebranded'' WAMI around two conceptions of sousveillance{\textemdash}``citizen-centered'' and ``state-centered''{\textemdash}creating an unlikely coalition of supporters from both pro- and anti-policing sides of the criminal justice reform debate. But while the renewed program has vowed to be a ``Big Brother'' to the state, it will continue to be used for traditional surveillance, raising troubling questions about privacy. The article sheds light on the politics of watching and being watched in the era of technology-driven criminal justice reform.},
  langid = {english},
  file = {/home/camille/Zotero/storage/YH5NWHHA/Snyder - 2020 - “Big Brother’s Bigger Brother” The Visual Politic.pdf}
}

@article{Snyder2021,
  title = {`{{All}} We See Is Dots': {{Aerial Objectivity}} and {{Mass Surveillance}} in {{Baltimore}}},
  shorttitle = {`{{All}} We See Is Dots'},
  author = {Snyder, Benjamin H.},
  year = {2021},
  month = oct,
  journal = {History of Photography},
  volume = {45},
  number = {3-4},
  pages = {376--387},
  publisher = {{Routledge}},
  issn = {0308-7298},
  doi = {10.1080/03087298.2022.2108263},
  urldate = {2023-09-11},
  abstract = {In 2020, the Baltimore Police Department used aerial surveillance camera technology, dubbed the `spy plane', that recorded the movements of nearly every citizen from above. Based on direct observation inside the programme's operations centre, this article shows how a `grainy truth' aesthetic, created by engineers to combat criticisms of the programme's invasiveness, also influenced the actual labour of surveillance. An obsession in the public debate and within the operations centre about how the imagery looks, however, overshadowed the most worrisome aspect of the programme: its infrastructure of representation. City officials are now saddled with managing a massive database of citizen location data owned by a private company, prompting difficult questions about the privatisation of policing.},
  keywords = {militarization,objectivity,policing,privatisation,surveillance,video}
}

@misc{T.C.L+2021a,
  title = {Towards Large-Scale, Automated, Accurate Detection of {{CCTV}} Camera Objects Using Computer Vision. {{Applications}} and Implications for Privacy, Safety, and Cybersecurity. ({{Preprint}})},
  author = {Turtiainen, Hannu and Costin, Andrei and Lahtinen, Tuomo and Sintonen, Lauri and Hamalainen, Timo},
  year = {2021},
  month = aug,
  number = {arXiv:2006.03870},
  eprint = {2006.03870},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2006.03870},
  urldate = {2023-09-16},
  abstract = {While the earliest known CCTV camera was developed almost a century ago back in 1927, currently, it is assumed as granted there are about 770 millions CCTV cameras around the globe, and their number is casually predicted to surpass 1 billion in 2021. Similarly to the first prototypes from 1927, at present the main promoted benefits for using and deploying CCTV cameras are physical security, safety, and prophylactics of crime. At the same time the increasing, widespread, unwarranted, and unaccountable use of CCTV cameras globally raises privacy risks and concerns for the last several decades. Recent technological advances implemented in CCTV cameras such as AI-based facial recognition and IoT connectivity only fuel further concerns raised by privacy advocates.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security},
  file = {/home/camille/Zotero/storage/YMYCSBK7/Turtiainen et al. - 2021 - Towards large-scale, automated, accurate detection.pdf}
}
